\documentclass[12pt, class=article, crop=false]{standalone}
\usepackage[subpreambles=true]{standalone}
\usepackage[T1]{fontenc} % for font setting
\usepackage{newtxtext,newtxmath}
\usepackage{import,
            graphicx,
            parskip,
            url,
            amsmath,
            wrapfig,
            fancyhdr,
            xcolor}
\usepackage[most]{tcolorbox}
\usepackage[top=2.54cm, bottom=2.54cm, left=2.54cm, right=2.54cm, marginparwidth=2cm]{geometry}%set margin
\usepackage[sort&compress]{natbib}
\setcitestyle{square}
\setcitestyle{comma}
\usepackage[font={sf, small}, labelfont={bf, small}]{caption}

\tcbuselibrary{breakable}
\bibliographystyle{bibstyle}
\graphicspath{{images/}}

% header
\pagestyle{fancy}
\fancyhead[L]{Data Management Plan}
\fancyhead[R]{}

\begin{document}
\section*{Data Management Plan}

\subsubsection*{Data/Code Management}

\textbf{Water Quality and Fish Data}: While we do not collect data in the field, we use field observations of stream chemistry and fish community compositions available in public data repositories. Data and metadata content and format will adhere to established standards, ensuring that the data are Findable, Accessible, Interoperable, and Reproducible (FAIR).

Throughout the project, data will exist in three forms.
\textit{Raw data}: This data is acquired immediately after collection from data repositories, prior to quality control and assurance checks.
\textit{Intermediate data}: Raw data that has undergone quality assurance checks.
It lists all raw data and flags data that did not pass quality checks, providing justification for exclusion.
\textit{Final data}: This is a subset of raw data considered of high enough quality for analysis.

The original data sourced from data repositories will be saved as spreadsheet (.csv) or R files (.rds) with ``raw'' in the filename.
Metadata from corresponding data repositories or through communications with data providers will be saved as a "README" markdown file (.md) in the same repository, allowing users to seamlessly access that information as needed.
\textit{Final data} will be hosted in Github along with source codes for analysis so readers can reproduce research findings.
We will track any changes to data using Github's version control system.

\textbf{Source Code}: We will develop and store our source codes for data formatting and statistical analysis in either R or Python scripts.
These scripts will be hosted on GitHub, ensuring version control and collaboration among project team members. 
GitHub's version control system will enable us to track changes, collaborate seamlessly, and maintain a historical record of code modifications throughout the project's duration.
To ensure that our research is accessible to the wider community, we commit to sharing our source codes as open-source. These codes will be made available under permissive licenses such as MIT or BSD. This allows others to use, modify, and build upon our work.

\textbf{Geospatial Data}: Our project heavily relies on Geospatial Information Systems (GIS). Geospatial data, which may include formats such as shapefiles, RDS (R Data Storage), or GeoTIFF, will be managed locally during the project.
The original data sourced from data repositories will be saved with ``raw'' in the filename.

\textbf{Data Storage}:
Our project will generate a substantial amount of digital data, for which we will employ a two-phase strategy to minimize the risk of data loss.
During the data analysis phase (the temporal phase), data will be stored locally with regular and automated backups to institutional cloud storage solutions, such as Departmental SharePoint at the University of North Carolina Greensboro (UNCG) or Research File Storage at the University of Kansas (KU). 
Currently, Terui (UNCG) and Hansen (KU) have access to 10TB and 15TB of institutional cloud storage space, respectively, with options to expand as needed (see \textit{Facilities \& Other Resources} for more details).

These institutional cloud storage systems are safeguarded by Redundant Array of Independent Disks (RAID) technology, which stores identical copies of data across multiple drives to protect against drive failures.
In the event of accidental deletion or data loss, recovery can be facilitated through self-service snapshots.
A snapshot is a ‘frozen,’ read-only view of a volume that allows easy access to previous versions of files and directories for up to 30 days. 

Upon completion of the analysis (the permanent phase), we will archive the data required to reproduce research findings in Zenodo.
Such datasets will include, but not be limited to, final water quality and/or fish data, environmental data derived from geospatial layers, and source codes for analysis.
Zenodo, a public repository that issues DOIs (Digital Object Identifiers) (https://zenodo.org/), provides a stable and citable location for data.
Total files size limit per record is 50GB, but higher quotas can be requested and granted on a case-by-case basis.
It also seamlessly integrates with GitHub, facilitating easy access to linked source code.
Additionally, Zenodo offers a version control system, enabling us to effectively manage any potential updates or modifications to published datasets as needed.
Zenodo boasts a rich history of utilization in various other scientific domains.

Each publication will have its own managed data package.
These data packages will be versioned, with the initial submission package labeled as v.1.0. Any changes after the initial submission will be clearly indicated by updating the version code (e.g., major changes will update the first digit, while minor changes will update the second digit), along with a separate change log that details the modifications in each version.

PD Terui is responsible for overseeing the permanent data packages.
Each PD is responsible for managing the source, intermediate, and final data copies related to their specific tasks:
\begin{itemize}
    \item University of Kansas (Hansen) -- 
    Data for and products from Objective 1.
    Hansen will use KU storage resources for data management as described above.
    \item UNCG (Terui) -- 
    Data for and products from Objectives 2 and 3.
    Terui will use UNCG storage resources for data management as described above.
    \item University of Minnesota (Dolph, Finlay) -- 
    Original fish data.
    Data is shared and co-managed with Terui.
    Terui will use UNCG storage resources for data management as described above.
    \item The Nature Conservancy (Kennedy, Harlan, Piazza) -- 
    Products used for online tools.
    Source products are shared and co-managed with Terui.
    Terui will use UNCG storage resources for data management as described above.
\end{itemize}

\subsubsection*{Policy for Product Access and Sharing}

\textbf{Repositories}: All project products will be publicly accessible through Zenodo, as indicated above.

\textbf{Publication and Timing}: We will share our data and source codes following the publication of major findings from our project or, at the latest, within three years after the project's completion, whichever comes first.
This timeline ensures that our work remains accessible to the public and can benefit the research community promptly.
We acknowledge that most of the datasets we will utilize are publicly available.
However, for any data that is state-owned and cannot be shared with the public, we will adhere to ethical and legal guidelines. We will provide references to the appropriate contacts, data archives, or publications that can guide readers on how to access or request the restricted data.
By adhering to these data management and sharing practices, our project aims to promote transparency, reproducibility, and open access to our computational resources and geospatial data.

\end{document}